# Optimizing Deep Learning Models on CIFAR-10

## **ðŸ“Œ Project Overview**
This project applies **advanced deep learning techniques** to improve **image classification accuracy** on the **CIFAR-10 dataset**. The notebook tests multiple **activation functions, weight initializers, optimizers, and regularization techniques** to determine the best approach for training neural networks.

### **ðŸš€ Key Features**
âœ… **Batch Normalization** â€“ Improves model stability and speeds up convergence  
âœ… **Activation Function Comparisons** â€“ **SELU vs. ReLU**  
âœ… **Transfer Learning** â€“ Freezing vs. unfreezing layers for efficient training  
âœ… **Optimizer Analysis** â€“ Comparing **Adam, SGD, AdaGrad, Nadam**  
âœ… **Regularization Techniques** â€“ L1/L2 Regularization, Dropout, Monte Carlo Dropout  
âœ… **Training Metrics Visualization** â€“ Accuracy and loss curves  

---

## **ðŸ“Œ Dataset: CIFAR-10**
- **CIFAR-10** consists of **60,000 color images** in **10 categories**, including:
  - Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck
- Each image is **32x32 pixels with 3 color channels**.

### **ðŸ“Œ Data Preprocessing**
- The dataset is **normalized** by scaling pixel values to `[0,1]`:
  ```python
  (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()
  X_train, X_test = X_train / 255.0, X_test / 255.0
